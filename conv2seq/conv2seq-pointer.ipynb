{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/songbohan/anaconda3/envs/reinforce_learning/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import logging\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from tensorflow.contrib import rnn\n",
    "from rouge import rouge\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add train log module\n",
    "log_fn = \"./log/train.20181022.log\"\n",
    "logger = logging.getLogger(\"training\")\n",
    "hdlr = logging.FileHandler(log_fn)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)\n",
    "tensorboard_log_path = \"./log/\"\n",
    "model_dump_dir=\"pointer\"\n",
    "# model_load_dir=\"../model/20180918\"\n",
    "inference_fn = \"./log/pointer.samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic_data_\n",
    "\n",
    "data_fn = \"../data/finance150.batch.pkl\"\n",
    "with open(data_fn, 'rb') as f:\n",
    "    abstract_train, abstract_test, title_train, title_test = pickle.load(open(data_fn, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameter\n",
    "\n",
    "beg, eos, emp, unk = 0, 1, 2, 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "save_epoc_step = 2\n",
    "dropout_keep_prob = 1.\n",
    "restore = False\n",
    "inference = False\n",
    "train = True\n",
    "batch_size = 128\n",
    "epocs = 1500\n",
    "\n",
    "maxlena = 200\n",
    "maxlent = 20\n",
    "maxlen = maxlena + maxlent\n",
    "maxlenh = maxlent\n",
    "maxlend = maxlena\n",
    "\n",
    "vocab_size = 80000\n",
    "# idx2word = {idx:word for word, idx in word2idx.items()}\n",
    "embedding_size = 100\n",
    "memory_dim = 512\n",
    "\n",
    "# cnn A, C\n",
    "filter_sizes = [3,3,3,3,3]\n",
    "num_filters = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Vocabulary class for mapping between words and ids (integers)\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, max_size):\n",
    "        \"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n",
    "\n",
    "        Args:\n",
    "          vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n",
    "          max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"\n",
    "        self._word_to_id = {}\n",
    "        self._id_to_word = {}\n",
    "        self._count = 0  # keeps track of total number of words in the Vocab\n",
    "\n",
    "        # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "        self._word_to_id[\"_unk\"] = unk\n",
    "        self._word_to_id[\"_beg\"] = beg\n",
    "        self._word_to_id[\"_eos\"] = eos\n",
    "        self._word_to_id[\"_emp\"] = emp\n",
    "        \n",
    "\n",
    "        # Read the vocab file and add words up to max_size\n",
    "        with open(vocab_file, 'r', encoding=\"utf-8\") as vocab_f:\n",
    "            for line in vocab_f:\n",
    "                w = line.strip().lower()\n",
    "                if w in self._word_to_id:\n",
    "                    print(\"Duplicate:\", w)\n",
    "                    continue\n",
    "                self._word_to_id[w] = self._count\n",
    "                self._id_to_word[self._count] = w\n",
    "                self._count += 1\n",
    "                if max_size != 0 and self._count >= max_size:\n",
    "                    print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (\n",
    "                    max_size, self._count))\n",
    "                    break\n",
    "\n",
    "        print(\"Finished constructing vocabulary of %i total words. Last ten word added: %s\" % (\n",
    "        self._count, \" \".join([self._id_to_word[self._count - i - 1] for i in range(10)])))\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"\n",
    "        if word not in self._word_to_id:\n",
    "            return self._word_to_id[\"_unk\"]\n",
    "        return self._word_to_id[word.lower()]\n",
    "\n",
    "    def id2word(self, word_id):\n",
    "        \"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"\n",
    "        if word_id not in self._id_to_word:\n",
    "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "        return self._id_to_word[word_id]\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"Returns the total size of the vocabulary\"\"\"\n",
    "        return self._count\n",
    "\n",
    "#     def write_metadata(self, fpath):\n",
    "#         \"\"\"Writes metadata file for Tensorboard word embedding visualizer as described here:\n",
    "#           https://www.tensorflow.org/get_started/embedding_viz\n",
    "\n",
    "#         Args:\n",
    "#           fpath: place to write the metadata file\n",
    "#         \"\"\"\n",
    "#         print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "#         with open(fpath, \"wb\") as f:\n",
    "#             fieldnames = ['word']\n",
    "#             writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "#             for i in range(self.size()):\n",
    "#                 writer.writerow({\"word\": self._id_to_word[i].encode(\"utf-8\")})\n",
    "\n",
    "#     def LoadWordEmbedding(self, w2v_file, word_dim):\n",
    "#         self.wordDict = {}\n",
    "#         self.word_dim = word_dim\n",
    "\n",
    "#         self.wordDict[UNKNOWN_TOKEN] = np.zeros(self.word_dim, dtype=np.float32)\n",
    "#         self.wordDict[PAD_TOKEN] = np.random.uniform(-1, 1, self.word_dim)\n",
    "#         self.wordDict[START_DECODING] = np.random.uniform(-1, 1, self.word_dim)\n",
    "#         self.wordDict[STOP_DECODING] = np.random.uniform(-1, 1, self.word_dim)\n",
    "#         with open(w2v_file) as wf:\n",
    "#             for line in wf:\n",
    "#                 info = line.strip().split()\n",
    "#                 word = info[0]\n",
    "#                 coef = np.asarray(info[1:], dtype='float32')\n",
    "#                 self.wordDict[word] = coef\n",
    "#                 assert self.word_dim == len(coef)\n",
    "\n",
    "#         self.MakeWordEmbedding()\n",
    "\n",
    "#     def MakeWordEmbedding(self):\n",
    "#         sorted_x = sorted(self._word_to_id.items(), key=operator.itemgetter(1))\n",
    "#         self._wordEmbedding = np.zeros((self.size(), self.word_dim),\n",
    "#                                        dtype=np.float32)  # replace unknown words with UNKNOWN_TOKEN embedding (zero vector)\n",
    "#         for word, i in sorted_x:\n",
    "#             if word in self.wordDict:\n",
    "#                 self._wordEmbedding[i, :] = self.wordDict[word.lower()]\n",
    "#         print('Word Embedding Reading done.')\n",
    "\n",
    "#     def getWordEmbedding(self):\n",
    "#         return self._wordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate: k\n",
      "Duplicate: n\n",
      "Duplicate: macd\n",
      "Duplicate: a\n",
      "Duplicate: e\n",
      "Duplicate: b\n",
      "Duplicate: wind\n",
      "Duplicate: app\n",
      "Duplicate: token\n",
      "Duplicate: c\n",
      "Duplicate: p2p\n",
      "Duplicate: kdj\n",
      "Duplicate: bp\n",
      "Duplicate: okex\n",
      "Duplicate: app\n",
      "Duplicate: fcoin\n",
      "Duplicate: l\n",
      "Duplicate: i\n",
      "Duplicate: wind\n",
      "Duplicate: x\n",
      "Duplicate: st\n",
      "Duplicate: 5g\n",
      "Duplicate: t\n",
      "Duplicate: eos\n",
      "Duplicate: v\n",
      "Duplicate: d\n",
      "Duplicate: shibor\n",
      "Duplicate: btc\n",
      "Duplicate: end\n",
      "Duplicate: boll\n",
      "Duplicate: end\n",
      "Duplicate: ipo\n",
      "Duplicate: pct\n",
      "Duplicate: pos\n",
      "Duplicate: pp\n",
      "Duplicate: w\n",
      "Duplicate: coindesk\n",
      "Duplicate: eth\n",
      "Duplicate: shibor\n",
      "Duplicate: hibor\n",
      "Duplicate: libor\n",
      "Duplicate: ps\n",
      "Duplicate: t0\n",
      "Duplicate: xx\n",
      "Duplicate: okex\n",
      "Duplicate: ok\n",
      "Duplicate: sto\n",
      "Duplicate: token\n",
      "Duplicate: choice\n",
      "Duplicate: okcoin\n",
      "Duplicate: 24h\n",
      "Duplicate: visa\n",
      "Duplicate: bitcoin\n",
      "Duplicate: usdt\n",
      "Duplicate: coinmarketcap\n",
      "Duplicate: msci\n",
      "Duplicate: g\n",
      "Duplicate: coindesk\n",
      "Duplicate: m\n",
      "Duplicate: vs\n",
      "Duplicate: one\n",
      "Duplicate: j\n",
      "Duplicate: p\n",
      "Duplicate: ico\n",
      "Duplicate: mt\n",
      "Duplicate: qq\n",
      "Duplicate: f\n",
      "Duplicate: pe\n",
      "Duplicate: abc\n",
      "Duplicate: beta\n",
      "Duplicate: alpha\n",
      "Duplicate: reits\n",
      "Duplicate: dapp\n",
      "Duplicate: ppt\n",
      "Duplicate: cdr\n",
      "Duplicate: coinmarketcap\n",
      "Duplicate: okex\n",
      "Duplicate: ii\n",
      "Duplicate: top10\n",
      "Duplicate: cointelegraph\n",
      "Duplicate: facebook\n",
      "Duplicate: bitfinex\n",
      "Duplicate: 1bp\n",
      "Duplicate: wifi\n",
      "Duplicate: o\n",
      "Duplicate: 5bp\n",
      "Duplicate: cboe\n",
      "Duplicate: bug\n",
      "Duplicate: dif\n",
      "Duplicate: r\n",
      "Duplicate: rsi\n",
      "Duplicate: fcoin\n",
      "Duplicate: com\n",
      "Duplicate: macd\n",
      "Duplicate: ppp\n",
      "Duplicate: comex\n",
      "Duplicate: io\n",
      "Duplicate: iot\n",
      "Duplicate: pta\n",
      "Duplicate: 2bp\n",
      "Duplicate: dapp\n",
      "Duplicate: pow\n",
      "Duplicate: dash\n",
      "Duplicate: 10w\n",
      "Duplicate: mm\n",
      "Duplicate: theend\n",
      "Duplicate: wifi\n",
      "Duplicate: fintech\n",
      "Duplicate: td\n",
      "Duplicate: etf\n",
      "Duplicate: allin\n",
      "Duplicate: 1w\n",
      "Duplicate: pdf\n",
      "Duplicate: logo\n",
      "Duplicate: sto\n",
      "Duplicate: s\n",
      "Duplicate: t1\n",
      "Duplicate: rmb\n",
      "Duplicate: ps\n",
      "Duplicate: or\n",
      "Duplicate: vivo\n",
      "Duplicate: pro\n",
      "Duplicate: tips\n",
      "Duplicate: okb\n",
      "Duplicate: 401k\n",
      "Duplicate: dai\n",
      "Duplicate: iii\n",
      "Duplicate: boss\n",
      "Duplicate: mr\n",
      "Duplicate: 3bp\n",
      "Duplicate: pos\n",
      "Duplicate: 10bp\n",
      "Duplicate: ma5\n",
      "Duplicate: github\n",
      "Duplicate: nasdaq\n",
      "Duplicate: delta\n",
      "Duplicate: ta\n",
      "Duplicate: id\n",
      "Duplicate: kd\n",
      "Duplicate: ios\n",
      "Duplicate: 4bp\n",
      "Duplicate: ht\n",
      "Duplicate: hk\n",
      "Duplicate: h\n",
      "Duplicate: y\n",
      "Duplicate: zerohedge\n",
      "Duplicate: vip\n",
      "Duplicate: psy\n",
      "Duplicate: coinbase\n",
      "Duplicate: ma\n",
      "Duplicate: eia\n",
      "Duplicate: zjh\n",
      "Duplicate: xxx\n",
      "Duplicate: fed\n",
      "Duplicate: twitter\n",
      "Duplicate: ma5\n",
      "Duplicate: facebook\n",
      "Duplicate: m3\n",
      "Duplicate: excel\n",
      "Duplicate: soc\n",
      "Duplicate: okex\n",
      "Duplicate: amp\n",
      "Duplicate: qtum\n",
      "Duplicate: m2\n",
      "Duplicate: pb\n",
      "Duplicate: wfee\n",
      "Duplicate: excel\n",
      "Duplicate: paypal\n",
      "Duplicate: ipv6\n",
      "Duplicate: gwh\n",
      "Duplicate: kw\n",
      "Duplicate: allin\n",
      "Duplicate: gdp\n",
      "Duplicate: no.1\n",
      "Duplicate: gate\n",
      "Duplicate: ofo\n",
      "Duplicate: st\n",
      "Duplicate: zf\n",
      "Duplicate: 5w\n",
      "Duplicate: bitmex\n",
      "Duplicate: u\n",
      "Duplicate: key\n",
      "Duplicate: cmcmarkets\n",
      "Duplicate: ccn\n",
      "Duplicate: nafta\n",
      "Duplicate: zan\n",
      "Duplicate: z\n",
      "Duplicate: iphone\n",
      "Duplicate: upbit\n",
      "Duplicate: vc\n",
      "Duplicate: dr\n",
      "Duplicate: zb\n",
      "Duplicate: kwh\n",
      "Duplicate: hadax\n",
      "Duplicate: top100\n",
      "Duplicate: ma10\n",
      "Duplicate: blockchain\n",
      "Duplicate: ma10\n",
      "Duplicate: cpi\n",
      "Duplicate: inc\n",
      "Duplicate: 6bp\n",
      "Duplicate: n1\n",
      "Duplicate: mac\n",
      "Duplicate: 100bp\n",
      "Duplicate: ltd\n",
      "Duplicate: 4h\n",
      "Duplicate: nano\n",
      "Duplicate: dea\n",
      "Duplicate: call\n",
      "Duplicate: roe\n",
      "Duplicate: boll\n",
      "Duplicate: 2w\n",
      "Duplicate: dapp\n",
      "Duplicate: pvc\n",
      "Duplicate: a2\n",
      "Duplicate: steem\n",
      "Duplicate: nongaap\n",
      "Duplicate: agk\n",
      "Duplicate: btm\n",
      "Duplicate: iost\n",
      "Duplicate: 20bp\n",
      "Duplicate: top\n",
      "Duplicate: bitcoinist\n",
      "Duplicate: bch\n",
      "Duplicate: fcoin\n",
      "Duplicate: gas\n",
      "Duplicate: dailyfx\n",
      "Duplicate: q\n",
      "Duplicate: top\n",
      "Duplicate: hof\n",
      "Duplicate: ripple\n",
      "Duplicate: co\n",
      "Duplicate: fx168\n",
      "Duplicate: call\n",
      "Duplicate: preipo\n",
      "Duplicate: saas\n",
      "Duplicate: cdn\n",
      "Duplicate: ont\n",
      "Duplicate: api\n",
      "Duplicate: cn\n",
      "Duplicate: pc\n",
      "Duplicate: iv\n",
      "Duplicate: cryptovest\n",
      "Duplicate: bnb\n",
      "Duplicate: gas\n",
      "Duplicate: so\n",
      "Duplicate: theta\n",
      "Duplicate: 100w\n",
      "Duplicate: adidas\n",
      "Duplicate: 20w\n",
      "Duplicate: coin\n",
      "Duplicate: tron\n",
      "Duplicate: kwh\n",
      "Duplicate: ta\n",
      "Duplicate: imtoken\n",
      "Duplicate: ada\n",
      "Duplicate: eos\n",
      "Duplicate: bitpay\n",
      "Duplicate: ing\n",
      "Duplicate: 7bp\n",
      "Duplicate: 25bp\n",
      "Duplicate: via\n",
      "Duplicate: tob\n",
      "Duplicate: ae\n",
      "Duplicate: news\n",
      "Duplicate: gjd\n",
      "Duplicate: low\n",
      "Duplicate: dapp\n",
      "Duplicate: youtube\n",
      "Duplicate: huobi\n",
      "Duplicate: yoy\n",
      "Duplicate: logo\n",
      "Duplicate: okcoin\n",
      "Duplicate: github\n",
      "Duplicate: 8bp\n",
      "Duplicate: 40bp\n",
      "Duplicate: top10\n",
      "Duplicate: box\n",
      "Duplicate: so\n",
      "Duplicate: sec\n",
      "Duplicate: yoy\n",
      "Duplicate: google\n",
      "Duplicate: infinox\n",
      "Duplicate: pro\n",
      "Duplicate: bigone\n",
      "Duplicate: out\n",
      "Duplicate: toc\n",
      "Duplicate: money\n",
      "Duplicate: bitshares\n",
      "Duplicate: kcash\n",
      "Duplicate: etc\n",
      "Duplicate: ai\n",
      "Duplicate: yy\n",
      "Duplicate: no\n",
      "Duplicate: carry\n",
      "Duplicate: line\n",
      "Duplicate: bigone\n",
      "Duplicate: no\n",
      "Duplicate: steam\n",
      "Duplicate: dfund\n",
      "Duplicate: vero\n",
      "Duplicate: kw\n",
      "Duplicate: soc\n",
      "Duplicate: bilibili\n",
      "Duplicate: oanda\n",
      "Duplicate: 50bp\n",
      "Duplicate: pk\n",
      "Duplicate: ipo3\n",
      "Duplicate: sz\n",
      "Duplicate: boss\n",
      "Duplicate: eco\n",
      "Duplicate: ft\n",
      "Duplicate: sir\n",
      "Duplicate: diff\n",
      "Duplicate: sky\n",
      "Duplicate: bug\n",
      "Duplicate: bitflyer\n",
      "Duplicate: brent\n",
      "Duplicate: zara\n",
      "Duplicate: neo\n",
      "Duplicate: 30w\n",
      "Duplicate: visa\n",
      "Duplicate: pass\n",
      "Duplicate: forex\n",
      "Duplicate: ma\n",
      "Duplicate: smartbeta\n",
      "Duplicate: top20\n",
      "Duplicate: followme\n",
      "Duplicate: liborois\n",
      "Duplicate: nvidia\n",
      "max_size of vocab was specified as 80000; we now have 80000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 80000 total words. Last ten word added: 自弹自唱 时且 狼会 膜技术 贏家 宝林 刘佳 显见 调和油 18.89\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\"../data/finance150.vocab\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article2ids(article_words, vocab):\n",
    "    \"\"\"Map the article words to their ids. Also return a list of OOVs in the article.\n",
    "\n",
    "    Args:\n",
    "      article_words: list of words (strings)\n",
    "      vocab: Vocabulary object\n",
    "\n",
    "    Returns:\n",
    "      ids:\n",
    "        A list of word ids (integers); OOVs are represented by their temporary article OOV number. If the vocabulary size is 50k and the article has 3 OOVs, then these temporary OOV numbers will be 50000, 50001, 50002.\n",
    "      oovs:\n",
    "        A list of the OOV words in the article (strings), in the order corresponding to their temporary article OOV numbers.\"\"\"\n",
    "    ids = []\n",
    "    oovs = []\n",
    "    for w in article_words:\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk:  # If w is OOV\n",
    "            if w not in oovs:  # Add to list of OOVs\n",
    "                oovs.append(w)\n",
    "            oov_num = oovs.index(w)  # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "            ids.append(vocab.size() + oov_num)  # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oovs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title2ids(title_words, vocab, article_oovs):\n",
    "    \"\"\"Map the title words to their ids. In-article OOVs are mapped to their temporary OOV numbers.\n",
    "\n",
    "    Args:\n",
    "      abstract_words: list of words (strings)\n",
    "      vocab: Vocabulary object\n",
    "      article_oovs: list of in-article OOV words (strings), in the order corresponding to their temporary article OOV numbers\n",
    "\n",
    "    Returns:\n",
    "      ids: List of ids (integers). In-article OOV words are mapped to their temporary OOV numbers. Out-of-article OOV words are mapped to the UNK token id.\"\"\"\n",
    "    ids = []\n",
    "    for w in title_words:\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk:  # If w is an OOV word\n",
    "            if w in article_oovs:  # If w is an in-article OOV\n",
    "                vocab_idx = vocab.size() + article_oovs.index(w)  # Map to its temporary article OOV number\n",
    "                ids.append(vocab_idx)\n",
    "            else:  # If w is an out-of-article OOV\n",
    "                ids.append(unk)  # Map to the UNK token id\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rpadd(x, maxlen=maxlenh, eos=eos, prefix=None):\n",
    "    \n",
    "    if prefix != None:\n",
    "        x = [prefix] + x\n",
    "    n = len(x)\n",
    "    if n > maxlen - 1:\n",
    "        x = x[:maxlen - 1]\n",
    "        n = maxlen - 1\n",
    "    res = x + [eos] + [emp] * (maxlen - 1 - n)\n",
    "    \n",
    "    assert len(res) == maxlen\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(titles_batch, article_batch):\n",
    "    \n",
    "    example_list = []\n",
    "    for i in range(len(titles_batch)):\n",
    "        article_ids, article_oovs = article2ids(article_batch[i], vocab)\n",
    "        title_ids = title2ids(titles_batch[i], vocab, article_oovs)\n",
    "        example_list.append((article_ids, title_ids, article_oovs))\n",
    "#         print(article_oovs)\n",
    "    max_art_oovs = max([len(ex[2]) for ex in example_list])\n",
    "    art_oovs = [ex[2] for ex in example_list]      \n",
    "    encoder_word_inputs = []\n",
    "    for i, ex in enumerate(example_list):\n",
    "        encoder_word_inputs.append(ex[0])\n",
    "    decoder_inputs = [ex[1] for ex in example_list]\n",
    "    \n",
    "    return max_art_oovs,  encoder_word_inputs, decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building model\n",
    "\n",
    "feed_batch_size = tf.placeholder(tf.int32, [], name=\"feed_batch_size\")\n",
    "encoder_word_inputs = tf.placeholder(tf.int32, shape=[None, maxlend], name=\"encoder_word_inputs\")\n",
    "encoder_pos_inputs = tf.placeholder(tf.int32, shape=[None, maxlend], name=\"encoder_pos_inputs\")\n",
    "decoder_targets = tf.placeholder(tf.int32, shape=[None, maxlenh], name=\"decoder_targets\")\n",
    "decoder_inputs = tf.placeholder(tf.int32, shape=[None, maxlenh], name=\"decoder_inputs\")\n",
    "max_art_oovs = tf.placeholder(tf.int32, [], name='max_art_oovs')\n",
    "\n",
    "\n",
    "writer = tf.summary.FileWriter(tensorboard_log_path, graph=tf.get_default_graph())\n",
    "\n",
    "\n",
    "# user_embeddings = tf.placeholder(tf.float32, shape=[vocab_size, embedding_size], name=\"user_embeddings\")\n",
    "word_embeddings = tf.get_variable(initializer=tf.truncated_normal(shape=[vocab_size, embedding_size], stddev=0.1), name=\"word_embeddings\")\n",
    "pos_embeddings = tf.Variable(tf.random_uniform([maxlend+3, embedding_size], -1,0, 1.0), name=\"pos_embeddings\")\n",
    "\n",
    "# writer = tf.summary.FileWriter(tensorboard_log_path, graph=tf.get_default_graph())\n",
    "\n",
    "\n",
    "encoder_word_inputs_embedded = tf.nn.embedding_lookup(word_embeddings, encoder_word_inputs)\n",
    "enocder_pos_inputs_embedded = tf.nn.embedding_lookup(pos_embeddings, encoder_pos_inputs)\n",
    "encoder_inputs_embedded = encoder_word_inputs_embedded + enocder_pos_inputs_embedded\n",
    "embed_expanded = tf.expand_dims(encoder_inputs_embedded, -1)\n",
    "\n",
    "def CNN_C_encoder(embed_expanded):\n",
    "        \n",
    "    cnn_c_inputs = embed_expanded\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.variable_scope(\"conv-c-%s\" % i):\n",
    "            filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "            W = tf.get_variable(initializer=tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.get_variable(initializer=tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(embed_expanded, W, strides=[1,1,1,1], padding=\"SAME\", name=\"conv\")\n",
    "            cnn_c_inputs = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            print(W.name)\n",
    "        \n",
    "    h = tf.squeeze(cnn_c_inputs, axis=3)\n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         h_drop = tf.nn.dropout(h, dropout_keep_prob, name=\"dropout\")\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv-c-0/W:0\n",
      "conv-c-1/W:0\n",
      "conv-c-2/W:0\n",
      "conv-c-3/W:0\n",
      "conv-c-4/W:0\n",
      "(?, 200, 100)\n"
     ]
    }
   ],
   "source": [
    "cnn_c_outputs = CNN_C_encoder(embed_expanded)\n",
    "print(cnn_c_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN_A_encoder(embed_expanded):\n",
    "    \n",
    "    cnn_a_inputs = embed_expanded\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.variable_scope(\"conv-a-%s\" % i):\n",
    "            filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "            W = tf.get_variable(initializer=tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.get_variable(initializer=tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(embed_expanded, W, strides=[1,1,1,1], padding=\"SAME\", name=\"conv\")\n",
    "            cnn_a_inputs = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "            print(W.name)\n",
    "        \n",
    "    h = tf.squeeze(cnn_a_inputs, axis=3)\n",
    "    \n",
    "#     with tf.name_scope(\"dropout\"):\n",
    "#         h_drop = tf.nn.dropout(h, dropout_keep_prob, name=\"dropout\")\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv-a-0/W:0\n",
      "conv-a-1/W:0\n",
      "conv-a-2/W:0\n",
      "conv-a-3/W:0\n",
      "conv-a-4/W:0\n",
      "(?, 200, 100)\n"
     ]
    }
   ],
   "source": [
    "cnn_a_outouts = CNN_A_encoder(embed_expanded)\n",
    "print(cnn_a_outouts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def LSTM_decoder(cnn_a_outouts, cnn_c_outputs, decoder_inputs):\n",
    "    \n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(word_embeddings, decoder_inputs)\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(memory_dim)\n",
    "    decoder_outputs = []\n",
    "    with tf.variable_scope(\"attetion\"):\n",
    "        state = lstm.zero_state(feed_batch_size, tf.float32)\n",
    "#         cell_init = tf.placeholder(tf.float32, shape=[None, memory_dim], name=\"init_cell\")\n",
    "#         hidden_init = tf.placeholder(tf.float32, shape=[None, memory_dim], name=\"init_hidden\")\n",
    "#         state = rnn.LSTMStateTuple(cell_init, hid_init)\n",
    "        W = tf.get_variable(initializer=tf.truncated_normal([memory_dim, embedding_size], stddev=0.1), name=\"W\")\n",
    "        b = tf.get_variable(initializer=tf.constant(0.1, shape=[embedding_size]), name=\"b\")\n",
    "        print(W.name)\n",
    "    \n",
    "    decoder_outputs = []\n",
    "    p_gens = []\n",
    "    attens = []\n",
    "\n",
    "#     cns = []\n",
    "    with tf.variable_scope(\"RNN\"):\n",
    "        for i in range(maxlenh): \n",
    "            d = tf.expand_dims(tf.matmul(state[1], W) + b + decoder_inputs_embedded[:,i,:], -1)\n",
    "            alpha = tf.expand_dims(tf.nn.softmax(tf.squeeze(tf.matmul(cnn_a_outouts, d), axis=-1), dim=1), axis=-1)\n",
    "            context = tf.reduce_sum(tf.multiply(alpha, cnn_c_outputs), axis=1)\n",
    "            current_input = tf.concat([context, decoder_inputs_embedded[:,i,:]], axis=1)\n",
    "\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables() \n",
    "\n",
    "            decoder_output, state = lstm(current_input, state)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            \n",
    "            attens.append(alpha)\n",
    "            with tf.variable_scope(\"calculate_pgen\"):\n",
    "                p_gen = tf.contrib.layers.fully_connected(tf.concat([current_input, state.c, state.h], axis=1), num_outputs=1, \n",
    "                                                          activation_fn=tf.sigmoid)\n",
    "                p_gens.append(p_gen)\n",
    "        \n",
    "#             cns.append(cnn_c_outputs)\n",
    "    return decoder_outputs, p_gens, attens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attetion/W:0\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs, p_gens, attens =  LSTM_decoder(cnn_a_outouts, cnn_c_outputs, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(200), Dimension(1)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_final_dist(v_size, _max_art_oovs, _enc_batch_extend_vocab, p_gen, vocab_dist, attn_dist):\n",
    "    \"\"\"Calculate the final distribution, for the pointer-generator model\n",
    "    Args:\n",
    "      vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.\n",
    "      attn_dists: The attention distributions. List length max_dec_steps of (batch_size, max_enc_steps) arrays\n",
    "\n",
    "    Returns:\n",
    "      final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('final_distribution'):\n",
    "        # Multiply vocab dists by p_gen and attention dists by (1-p_gen)\n",
    "        vocab_dist = p_gen * vocab_dist\n",
    "        attn_dist = (1 - p_gen) * attn_dist\n",
    "\n",
    "        # Concatenate some zeros to each vocabulary dist, to hold the probabilities for in-article OOV words\n",
    "        extended_vsize = v_size + _max_art_oovs  # the maximum (over the batch) size of the extended vocabulary\n",
    "        extra_zeros = tf.zeros((feed_batch_size, _max_art_oovs))\n",
    "        vocab_dists_extended = tf.concat(axis=1, values=[vocab_dist,\n",
    "                                                         extra_zeros])  # list length max_dec_steps of shape (batch_size, extended_vsize)\n",
    "\n",
    "        # Project the values in the attention distributions onto the appropriate entries in the final distributions\n",
    "        # This means that if a_i = 0.1 and the ith encoder word is w, and w has index 500 in the vocabulary, then we add 0.1 onto the 500th entry of the final distribution\n",
    "        # This is done for each decoder timestep.\n",
    "        # This is fiddly; we use tf.scatter_nd to do the projection\n",
    "        batch_nums = tf.range(0, limit=feed_batch_size)  # shape (batch_size)\n",
    "        batch_nums = tf.expand_dims(batch_nums, 1)  # shape (batch_size, 1)\n",
    "        attn_len = tf.shape(_enc_batch_extend_vocab)[1]  # number of states we attend over\n",
    "        batch_nums = tf.tile(batch_nums, [1, attn_len])  # shape (batch_size, attn_len)\n",
    "        indices = tf.stack((batch_nums, _enc_batch_extend_vocab), axis=2)  # shape (batch_size, enc_t, 2)\n",
    "        shape = [feed_batch_size, extended_vsize]\n",
    "        attn_dists_projected = tf.scatter_nd(indices, attn_dist,\n",
    "                                             shape)  # list length max_dec_steps (batch_size, extended_vsize)\n",
    "\n",
    "        # Add the vocab distributions and the copy distributions together to get the final distributions\n",
    "        # final_dists is a list length max_dec_steps; each entry is a tensor shape (batch_size, extended_vsize) giving the final distribution for that decoder timestep\n",
    "        # Note that for decoder timesteps and examples corresponding to a [PAD] token, this is junk - ignore.\n",
    "        final_dist = vocab_dists_extended + attn_dists_projected\n",
    "        final_dist += 1e-15  # for cases where we have zero in the final dist, especially for oov words\n",
    "        dist_sums = tf.reduce_sum(final_dist, axis=1)\n",
    "        final_dist = final_dist / tf.reshape(dist_sums, [-1, 1])  # re-normalize\n",
    "\n",
    "    return final_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def feed_forward(decoder_outputs, p_gens, attens):\n",
    "    final_dists = []\n",
    "    with tf.variable_scope(\"FNN\"):\n",
    "        w = tf.get_variable(name=\"W\", initializer=tf.truncated_normal(shape=[memory_dim, vocab_size], stddev=0.1))\n",
    "        b = tf.get_variable(name=\"b\", initializer=tf.constant(0.1, shape=[vocab_size]))\n",
    "    with tf.variable_scope(\"feed_forward\") as scope:   \n",
    "        for i in range(maxlent):\n",
    "            decoder_output, p_gen, atten = decoder_outputs[i], p_gens[i], tf.squeeze(attens[i], axis=-1)\n",
    "            \n",
    "            decoder_logit = tf.nn.softmax(tf.matmul(decoder_output, w) + b)\n",
    "            final_dist = calc_final_dist(vocab_size, max_art_oovs, encoder_word_inputs, p_gen, decoder_logit, atten)\n",
    "            final_dists.append(final_dist)\n",
    "    return final_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dists = feed_forward(decoder_outputs, p_gens, attens)\n",
    "stepwise_cross_entropys = []\n",
    "decoder_targets_list = tf.unstack(decoder_targets, axis=1)\n",
    "for i in range(maxlent):\n",
    "    \n",
    "    label = tf.one_hot(decoder_targets_list[i], depth=max_art_oovs+vocab_size, dtype=tf.float32)\n",
    "    final_dist = final_dists[i]\n",
    "    stepwise_cross_entropys.append(tf.reduce_sum(-label * tf.log(final_dist))/tf.cast(feed_batch_size, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.stack(stepwise_cross_entropys, axis=0))\n",
    "decoder_prediction = tf.argmax(tf.stack(final_dists, axis=0), 2, name=\"decoder_prediction\")\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, name=\"op_adam_minize\")\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name word_embeddings:0 is illegal; using word_embeddings_0 instead.\n",
      "INFO:tensorflow:Summary name pos_embeddings:0 is illegal; using pos_embeddings_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-0/W:0 is illegal; using conv-c-0/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-0/b:0 is illegal; using conv-c-0/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-1/W:0 is illegal; using conv-c-1/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-1/b:0 is illegal; using conv-c-1/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-2/W:0 is illegal; using conv-c-2/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-2/b:0 is illegal; using conv-c-2/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-3/W:0 is illegal; using conv-c-3/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-3/b:0 is illegal; using conv-c-3/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-4/W:0 is illegal; using conv-c-4/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-c-4/b:0 is illegal; using conv-c-4/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-0/W:0 is illegal; using conv-a-0/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-0/b:0 is illegal; using conv-a-0/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-1/W:0 is illegal; using conv-a-1/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-1/b:0 is illegal; using conv-a-1/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-2/W:0 is illegal; using conv-a-2/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-2/b:0 is illegal; using conv-a-2/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-3/W:0 is illegal; using conv-a-3/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-3/b:0 is illegal; using conv-a-3/b_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-4/W:0 is illegal; using conv-a-4/W_0 instead.\n",
      "INFO:tensorflow:Summary name conv-a-4/b:0 is illegal; using conv-a-4/b_0 instead.\n",
      "INFO:tensorflow:Summary name attetion/W:0 is illegal; using attetion/W_0 instead.\n",
      "INFO:tensorflow:Summary name attetion/b:0 is illegal; using attetion/b_0 instead.\n",
      "INFO:tensorflow:Summary name RNN/basic_lstm_cell/kernel:0 is illegal; using RNN/basic_lstm_cell/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name RNN/basic_lstm_cell/bias:0 is illegal; using RNN/basic_lstm_cell/bias_0 instead.\n",
      "INFO:tensorflow:Summary name RNN/calculate_pgen/fully_connected/weights:0 is illegal; using RNN/calculate_pgen/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name RNN/calculate_pgen/fully_connected/biases:0 is illegal; using RNN/calculate_pgen/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name FNN/W:0 is illegal; using FNN/W_0 instead.\n",
      "INFO:tensorflow:Summary name FNN/b:0 is illegal; using FNN/b_0 instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"summary\"):\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.histogram(\"p_gens\", tf.stack(p_gens, axis=0))\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.name, var)\n",
    "    for i, ce in enumerate(stepwise_cross_entropys):\n",
    "        tf.summary.scalar(\"stepwise_cross_entropy\", ce)\n",
    "    for final_dist in final_dists:\n",
    "        tf.summary.histogram(\"final_dist\", final_dist)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ids2sent(x):\n",
    "    sent = []\n",
    "    for idx in x:\n",
    "#         if idx == emp:\n",
    "#             continue\n",
    "        if idx == eos:\n",
    "            break\n",
    "        sent.append(idx2word[idx])\n",
    "    \n",
    "    return \" \".join(sent)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_test, Y_test, sess, log_dir):\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    rouge_mea = {\n",
    "        'rouge_1/f_score': 0.0,\n",
    "        'rouge_1/r_score': 0.0,\n",
    "        'rouge_1/p_score': 0.0,\n",
    "        'rouge_2/f_score': 0.0,\n",
    "        'rouge_2/r_score': 0.0,\n",
    "        'rouge_2/p_score': 0.0,\n",
    "        'rouge_l/f_score': 0.0,\n",
    "        'rouge_l/r_score': 0.0,\n",
    "        'rouge_l/p_score': 0.0\n",
    "    }\n",
    "    outf = open(log_dir, 'w', encoding='utf-8')\n",
    "    j = 0\n",
    "    while j < len(X_test):\n",
    "        \n",
    "        titles_batch, abstracts_batch = title_train[j:j+batch_size], abstract_train[j:j+batch_size]\n",
    "        max_art_oovs_, X_train, Y_train = make_batch(titles_batch, abstracts_batch)\n",
    "        encoder_word_inputs_ = list(map(lambda x: rpadd(x, maxlend), X_train))\n",
    "        encoder_pos_inputs_ = list(map(lambda x:rpadd(list(range(3,len(x)+3)), maxlend), X_train))\n",
    "        decoder_inputs_ = list(map(lambda x: rpadd(x, maxlenh, prefix=beg), Y_train))\n",
    "        decoder_targets_ = list(map(lambda x: x[1:]+[emp], decoder_inputs_))\n",
    "        j = j + batch_size\n",
    "        \n",
    "      \n",
    "        test_x = [[] for _ in range(len(encoder_word_inputs_))]\n",
    "        for l in range(maxlenh):\n",
    "            new_decoder_input_ = list(map(lambda x: rpadd(x, maxlenh, prefix=beg), test_x))\n",
    "\n",
    "            decoder_prediction_= sess.run(decoder_prediction,\n",
    "                                          feed_dict={\n",
    "                                              encoder_word_inputs: encoder_word_inputs_,\n",
    "                                              encoder_pos_inputs: encoder_pos_inputs_,\n",
    "                                              decoder_inputs: new_decoder_input_,\n",
    "                                              feed_batch_size: 1,\n",
    "                                              max_art_oovs: max_art_oovs_\n",
    "                                          })\n",
    "            for b in range(len(encoder_word_inputs_)):\n",
    "                next_word_idx = decoder_prediction_[b][l]\n",
    "                test_x[b].append(next_word_idx)\n",
    "                        \n",
    "                \n",
    "        # compute 'rouge' measure in the batch\n",
    "        hypotheses = [ids2sent(single) for single in test_x]\n",
    "        references = [ids2sent(single) for single in decoder_inputs_]\n",
    "        \n",
    "        rouge_batch = rouge(hypotheses, references)\n",
    "        \n",
    "        for key in rouge_mea.keys():\n",
    "            rouge_mea[key] += rouge_batch[key]\n",
    "\n",
    "        # show batch result in file \n",
    "        for b in range(len(encoder_word_inputs_)):\n",
    "            \n",
    "            outf.write(\"{0}\\t{1}\\t{2}\\n\".format(ids2sent(test_x[b]), ids2sent(decoder_inputs_[b])\n",
    "                                                , ids2sent(encoder_word_inputs_[b])))\n",
    "            \n",
    "    # compute rouge measue in the test\n",
    "    outf.write(\"------rouge measure------\")\n",
    "    for key in rouge_mea.keys():\n",
    "        rouge_mea[key] = rouge_mea[key] / (len(X_test)//batch_size)\n",
    "        outf.write(\"{0}: {1}\\n\".format(key, rouge_mea[key]))\n",
    "\n",
    "    \n",
    "    outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start trainging...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "if restore:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"./ckpt/\"))\n",
    "if inference:\n",
    "    predict(X_test, Y_test, sess, inference_fn)\n",
    "\n",
    "if train:\n",
    "    print(\"start trainging...\")  \n",
    "    for i in range(epocs):\n",
    "        j = 0\n",
    "        while j < len(title_train):\n",
    "            \n",
    "            titles_batch, abstracts_batch = title_train[j:j+batch_size], abstract_train[j:j+batch_size]\n",
    "            max_art_oovs_, X_train, Y_train = make_batch(titles_batch, abstracts_batch)\n",
    "            encoder_word_inputs_ = list(map(lambda x: rpadd(x, maxlend), X_train))\n",
    "            encoder_pos_inputs_ = list(map(lambda x:rpadd(list(range(3,len(x)+3)), maxlend), X_train))\n",
    "            decoder_inputs_ = list(map(lambda x: rpadd(x, maxlenh, prefix=beg), Y_train))\n",
    "            decoder_targets_ = list(map(lambda x: x[1:]+[emp], decoder_inputs_))\n",
    "            j = j + batch_size\n",
    "            summary, _, loss_, decoder_prediction_ = sess.run([summary_op, train_op, loss, decoder_prediction],\n",
    "                                                             feed_dict={\n",
    "                                                                 encoder_word_inputs: encoder_word_inputs_,\n",
    "                                                                 encoder_pos_inputs: encoder_pos_inputs_,\n",
    "                                                                 decoder_inputs: decoder_inputs_,\n",
    "                                                                 decoder_targets: decoder_targets_,\n",
    "                                                                 feed_batch_size: len(encoder_word_inputs_),\n",
    "                                                                 max_art_oovs: max_art_oovs_\n",
    "                                                             })\n",
    "            writer.add_summary(summary, i*len(X_train) + j)\n",
    "            \n",
    "            # debug\n",
    "            A, B = sess.run([final_dists[0], tf.one_hot(decoder_targets_list[0], depth=max_art_oovs+vocab_size, dtype=tf.float32)], \n",
    "                           feed_dict={\n",
    "                                 encoder_word_inputs: encoder_word_inputs_,\n",
    "                                 encoder_pos_inputs: encoder_pos_inputs_,\n",
    "                                 decoder_inputs: decoder_inputs_,\n",
    "                                 decoder_targets: decoder_targets_,\n",
    "                                 feed_batch_size: len(encoder_word_inputs_),\n",
    "                                 max_art_oovs: max_art_oovs_\n",
    "                             })\n",
    "    \n",
    "            if j % (batch_size * 30) == 0:\n",
    "                logger.info(\"Runing in epoc {0} batch {1} with loss {2}\".format(i, j//batch_size, loss_))\n",
    "\n",
    "\n",
    "        if i % save_epoc_step == 0:\n",
    "            saver.save(sess, \"./ckpt/{0}\".format(model_dump_dir), global_step=i)\n",
    "    print(\"finish trainging.\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "titles_batch, abstracts_batch = title_train[j:j+batch_size], abstract_train[j:j+batch_size]\n",
    "max_art_oovs_,  X_train, Y_train = make_batch(titles_batch, abstracts_batch)\n",
    "decoder_inputs_ = list(map(lambda x: rpadd(x, maxlenh, prefix=beg), Y_train))\n",
    "decoder_targets_ = list(map(lambda x: x[1:]+[emp], decoder_inputs_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets = tf.constant(value=decoder_inputs_)\n",
    "decoder_targets_list = tf.unstack(decoder_targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_art_oovs = tf.constant(value=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.one_hot(decoder_targets_list[0], depth=max_art_oovs+vocab_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cite: https://arxiv.org/pdf/1702.01806.pdf\n",
    "# title: Beam Search Strategies for Neural Machine Translation\n",
    "# beam search with pruning (naive, version)\n",
    "# prune hypo parameter\n",
    "\n",
    "rp = 0.2\n",
    "ap = 2.5\n",
    "rpl = 0.3\n",
    "mc = 2\n",
    "beam_size = 30000\n",
    "\n",
    "def prune(beam_tmp, decoder_probs_l):\n",
    "    \n",
    "#     print([[idx2word[idx] for idx in seq] for score, seq in beam_tmp])\n",
    "    \n",
    "#     max_score, max_seq = beam_tmp[0]\n",
    "#     print(\"max_score: {0}, max_seq: {1}\".format(max_score, max_seq))\n",
    "#     beam_tmp = list(filter(lambda x: x[0] > rp * max_score, beam_tmp))\n",
    "#     beam_tmp = list(filter(lambda x: x[0] > max_score - ap, beam_tmp))\n",
    "#     beam_tmp = list(filter(lambda x: decoder_probs_l[x[1][-1]] > rpl * decoder_probs_l[max_seq[-1]], beam_tmp))\n",
    "    \n",
    "#     beam_res = []\n",
    "#     cnt = {}\n",
    "#     for prob, seq in beam_tmp:\n",
    "#         key = \"_\".join([str(s) for s in seq])\n",
    "#         if key not in cnt:\n",
    "#             cnt[key] = 0\n",
    "#         if cnt[key] < mc:\n",
    "#             beam_res.append((prob, seq))\n",
    "#             cnt[key] += 1\n",
    "            \n",
    "#     print([[idx2word[idx] for idx in seq] for score, seq in beam_res])\n",
    "\n",
    "    \n",
    "    \n",
    "    return beam_res\n",
    "\n",
    "def beam_search_with_pruning(X_one, Y_one, sess, beam_size):\n",
    "    \n",
    "    encoder_word_inputs_ = [rpadd(X_one, maxlend)]\n",
    "    encoder_pos_inputs_ = [rpadd(list(range(3,len(X_one)+3)), maxlend)]\n",
    "    \n",
    "    beam_list = [(1.0, [beg])] \n",
    "    final_list = []\n",
    "    \n",
    "    for l in range(maxlenh):\n",
    "\n",
    "        candidate = {}\n",
    "        for prob_seq, seq in beam_list:\n",
    "            \n",
    "            \n",
    "            new_decoder_input_ = [rpadd(seq, maxlenh, prefix=None)]\n",
    "   \n",
    "            decoder_logits_ = sess.run(tf.log(tf.stack()),\n",
    "                                          feed_dict={\n",
    "                                              encoder_word_inputs: encoder_word_inputs_,\n",
    "                                              encoder_pos_inputs: encoder_pos_inputs_,\n",
    "                                              decoder_inputs: new_decoder_input_,\n",
    "                                              feed_batch_size: 1,\n",
    "                                              max_art_oovs: max_art_oovs_    \n",
    "                                          })\n",
    "            decoder_probs_l = decoder_logits_[0][l]\n",
    "            probs_top_ids = np.argsort(decoder_probs_l)[-beam_size:]\n",
    "#             print([idx2word[idx] for idx in probs_top_ids])\n",
    "            \n",
    "#             print(\"------------------------------\")\n",
    "            \n",
    "            for v in probs_top_ids:\n",
    "                if v == unk or v == emp: continue\n",
    "                pt = prob_seq + decoder_probs_l[v]\n",
    "                candidate[pt] = seq + [v]\n",
    "                \n",
    "       \n",
    " \n",
    "        res_sort = sorted(list(candidate.items()), key=lambda x: x[0], reverse=True)   \n",
    "        \n",
    "        beam_tmp = res_sort[:beam_size]\n",
    "        \n",
    "        # beam_tmp = prune(beam_tmp, decoder_probs_l)  \n",
    "                \n",
    "        beam_list.clear()\n",
    "        \n",
    "        for prob, seq in beam_tmp:\n",
    "            if seq[-1] == eos:\n",
    "                final_list.append((prob, seq))\n",
    "                beam_size -= 1\n",
    "            else:\n",
    "                beam_list.append((prob, seq))\n",
    "            \n",
    "        if len(beam_list) == 0 or beam_size == 0:\n",
    "            break\n",
    "           \n",
    "    return [(ids2sent(item[1]), item[0]) for item in final_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = beam_search_with_pruning(X_test[3], Y_test[3], sess, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_with_beam(beam_size=4, filename=\"./log/produce.beam.samples\", sess=sess, X_test=X_test, Y_test=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids2sent(X_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids2sent(Y_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_beam(X_test, Y_test, sess, beam_size, filename):\n",
    "    rouge_mea = {\n",
    "        'rouge_1/f_score': 0.0,\n",
    "        'rouge_1/r_score': 0.0,\n",
    "        'rouge_1/p_score': 0.0,\n",
    "        'rouge_2/f_score': 0.0,\n",
    "        'rouge_2/r_score': 0.0,\n",
    "        'rouge_2/p_score': 0.0,\n",
    "        'rouge_l/f_score': 0.0,\n",
    "        'rouge_l/r_score': 0.0,\n",
    "        'rouge_l/p_score': 0.0\n",
    "    }\n",
    "    \n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    out = open(filename, \"w\", encoding=\"utf-8\")\n",
    "    for x, y in zip(X_test, Y_test):\n",
    "        res = beam_search_with_pruning(x, y, sess, beam_size)[0][0]\n",
    "        out.write(\"{0}\\t{1}\\t{2}\\n\".format(res, ids2sent(y), ids2sent(x)))\n",
    "        out.flush()\n",
    "    \n",
    " \n",
    "        hypotheses.append(ids2sent(x))\n",
    "        references.append(res)\n",
    "        \n",
    "    rouge_mea = rouge(hypotheses, references)\n",
    "\n",
    "    # compute rouge measue in the test\n",
    "    outf.write(\"------rouge measure------\")\n",
    "    for key in rouge_mea.keys():\n",
    "        rouge_mea[key] = rouge_mea[key] / (len(X_test)//batch_size)\n",
    "        outf.write(\"{0}: {1}\\n\".format(key, rouge_mea[key]))\n",
    "\n",
    "    \n",
    "    outf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_online(X_test, Y_test, sess, test_content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_proper(t, test_content_words[3], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\" \".join([w for w in test_content_words[0].split(\" \") if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real = []\n",
    "for word in can.split(\" \")[1:]:\n",
    "    if word in test_content_ner2words[1]:\n",
    "        real.append(test_content_ner2words[1][word][0])\n",
    "    else:\n",
    "        real.append(word)\n",
    "print(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_content_ner2words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a in t:\n",
    "    r = rouge([a[0]], [test_content_words[3]])\n",
    "    print(a[0])\n",
    "    print(test_content_words[3])\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "1 - scipy.spatial.distance.cosine(embed[word2idx[\"你\"]],embed[word2idx[\"我\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试词向量\n",
    "\n",
    "def neighbor(target_word, k):\n",
    "    \n",
    "    w2sim = {}\n",
    "    for word in word2idx.keys():\n",
    "        sim = 1 - scipy.spatial.distance.cosine(embed[word2idx[target_word]],embed[word2idx[word]]) \n",
    "        w2sim[word] = sim\n",
    "    \n",
    "    return sorted(w2sim.items(), key=lambda x:x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns = neighbor(\"外汇\", 20)\n",
    "for n in ns:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reinforce_learning]",
   "language": "python",
   "name": "conda-env-reinforce_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
