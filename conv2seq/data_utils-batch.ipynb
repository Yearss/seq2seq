{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, val, tf, df):\n",
    "        self.val = val\n",
    "        self.tf = tf\n",
    "        self.df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_fname):\n",
    "    abstracts, titles = [], []\n",
    "    vocab = set()\n",
    "    with open(data_fname, 'r', encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx % 50000 == 0: print(\"progress:{0}\".format(idx))\n",
    "            try:\n",
    "                title, abstract = line.strip().split(\"\\t\")\n",
    "            except:\n",
    "                continue\n",
    "            titles.append(title.split(\" \"))\n",
    "            abstracts.append(abstract.split(\" \"))\n",
    "            vocab.update(title.split(\" \"), abstract.split(\" \"))\n",
    "    return titles, abstracts, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress:0\n",
      "progress:50000\n",
      "progress:100000\n",
      "progress:150000\n",
      "progress:200000\n",
      "progress:250000\n",
      "progress:350000\n",
      "progress:400000\n",
      "progress:450000\n",
      "progress:500000\n",
      "progress:550000\n",
      "progress:650000\n",
      "progress:700000\n",
      "progress:750000\n",
      "progress:800000\n",
      "progress:850000\n",
      "progress:900000\n",
      "progress:950000\n",
      "progress:1000000\n",
      "progress:1050000\n",
      "progress:1100000\n",
      "progress:1150000\n",
      "progress:1200000\n",
      "progress:1250000\n",
      "progress:1300000\n",
      "progress:1350000\n",
      "progress:1400000\n",
      "progress:1450000\n",
      "progress:1500000\n",
      "Got 1513342 doc in corpus\n"
     ]
    }
   ],
   "source": [
    "data_fname = \"../data/processed.s2s.v1.txt\"\n",
    "titles, abstracts, vocab = read_data(data_fname)\n",
    "print(\"Got {0} doc in corpus\".format(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_word_tf_df(corpus):\n",
    "    word_set = {}\n",
    "    for doc in corpus:\n",
    "        title, abstract = doc[0], doc[1]\n",
    "        words = title + abstract\n",
    "        for w in words:\n",
    "            if w not in word_set:\n",
    "                word_set[w] = Word(val=w, tf=1, df=0)\n",
    "            else:\n",
    "                word_set[w].tf += 1\n",
    "        for w in set(words):\n",
    "            word_set[w].df += 1\n",
    "\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_idx_for_word_tf_df(word_set, tf_thres=12, df_thres=6):\n",
    "        \n",
    "    word_list = list(filter(lambda w: w.tf > tf_thres and w.df > df_thres, word_set.values()))\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2663659 unique word\n"
     ]
    }
   ],
   "source": [
    "corpus = zip(titles, abstracts)\n",
    "word_set = cal_word_tf_df(corpus)\n",
    "print(\"Got {0} unique word\".format(len(word_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 10 are: \n",
      "，\t87890784\t1493097\n",
      "的\t49538168\t1471616\n",
      "。\t33842224\t1478923\n",
      "、\t15220155\t1191988\n",
      "在\t10334975\t1336449\n",
      "是\t10055198\t1259035\n",
      "了\t8448966\t1223957\n",
      "：\t6286885\t1183965\n",
      "和\t5949918\t1134375\n",
      "年\t4675176\t811846\n",
      "X_train length: 1512342\n",
      "X_test length: 1000\n",
      "Y_train length: 1512342\n",
      "Y_test length: 1000\n"
     ]
    }
   ],
   "source": [
    "word_list = build_idx_for_word_tf_df(word_set)\n",
    "top_tf_words = sorted(word_list, key=lambda x: x.tf, reverse=True)\n",
    "print(\"The Top 10 are: \")\n",
    "print(\"\\n\".join([\"{0}\\t{1}\\t{2}\".format(word.val, word.tf, word.df) for word in top_tf_words[:10]]))\n",
    "abstract_train, abstract_test, title_train, title_test = train_test_split(abstracts, titles, test_size=1000)\n",
    "print(\"X_train length: {0}\\nX_test length: {1}\\nY_train length: {2}\\nY_test length: {3}\".format(len(abstract_train),\n",
    "                                                                                                len(abstract_test),\n",
    "                                                                                                len(title_train),\n",
    "                                                                                                len(title_test)\n",
    "                                                                                                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_of_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cec84e5596cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab size: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvocab_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/finance150.vocab\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_of_words' is not defined"
     ]
    }
   ],
   "source": [
    "data_fn = \"../data/finance150.batch.pkl\"\n",
    "with open(data_fn, 'wb') as f:\n",
    "    pickle.dump((abstract_train, abstract_test, title_train, title_test), f, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 378482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['来自', '外汇交易', '高手', '的', '三点', '“', '忠告', '”']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"vocab size: {0}\".format(len(top_tf_words)))\n",
    "vocab_fn = \"../data/finance150.vocab\"\n",
    "with open(vocab_fn, \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in top_tf_words:\n",
    "        f.write(w.val+\"\\n\")\n",
    "        f.flush()\n",
    "idx = random.randint(0, len(abstract_train))\n",
    "abstract_train[idx]\n",
    "title_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
